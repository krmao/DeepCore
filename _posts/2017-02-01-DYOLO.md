---
layout: post
title: YOLOD - Improving CNN-based object detection for tightly packed cars
author: Alan J. Schoen
author_title: Data Scientist
published: true
desc: Adding deconvolutions to YOLOv2 improves performance with tightly packed cars.
keywords: machine learning, remote sensing, satellite imagery, deep learning, convolutional neural networks
---

Neural network research moves very quickly and it can be hard to keep up.  In our work at Radiant Solutions, we use a variety of machine learning modalities.  My team works with many types of models, and many different types of neural networks, but our main focus is using convolutional neural networks for object detection.  Detection is just the technical term for drawing boxes around objects of interests.  For instance, we might want to find all the cars in an image and draw boxes around them.  We're fortunate to be able to borrow a lot of techniques from academic research, but sometimes academic research doesn't line up perfectly with our needs.

Most research on object detectors is developed the same training sets, like PASCAL-VOC and MS-COCO.  Both of these datasets are made of up what you might call "normal" pictures, taken with a normal camera a few feet off the ground.  At Radiant Solutions, we work with a particular type of picture, taken with a specialized camera at a vantage point about 600 kilometers above the earth's surface (read more about DigitalGlobe satellites [here](https://www.digitalglobe.com/about/our-constellation)).  As a result, there are systematic differences between the images in PASCAL-VOC and our training data.  For instance, we have to deal with parking lots, where cars are packed in right next to each other.  Most detection neural networks are terrible for counting cars in satellite imagery.

YOLO is one of the best networks out there for object detection in PASCAL VOC, and we have done a lot of research using YOLO.  Our colleagues at [CosmiQ Works](https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571) improved on the YOLO design to get better performance with satellite images.  Continuing in that tradition, we decided to innovate on [YOLOv2](https://arxiv.org/abs/1612.08242), with inspiration from anther network called [DSSD](https://arxiv.org/abs/1701.06659).

Convolutional neural networks work by basically squeezing information out of images.  Usually, they alternate between convolutional layers that search for patterns in an image, and pooling layers that throw away irrelevant information.  The overall effect is that as we move through a neural network, the information is concentrated into a smaller representation.  It's like the famous quote about how to turn a stone into a statue of an elephant: "It is easy, you just chip away all the bits that don't look like an elephant.".  So in general your information shrinks as it passes through a neural network.  In the case of YOLOv2, we start with a 300 pixel by 300 pixel image, and by the end of the network we have a 13x13 grid for object detection.  This works great when you only have a few objects in each image, but it doesn't work as well when the image is packed with objects, like in a parking lot.  For practical reasons, which I won't try to explain here, you really don't want a detector to have to deal with a lot of objects in each detection grid cell.

The simplest solution to this is to just use smaller input images and resize them to 416x416, but then we're throwing away context.  It's easier to tell that a car is a car if you can see that it's in a parking lot.  So we want a model that can see context and detail at the same time.

There are several ways to address this issue.  [YOLT (You Only Look Twice)](https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571) uses a passthrough layer so that the network can combine detailed information early in the network with highly processed information later in the network. This leads to a marked improvement in detection of small objects, and the second version of YOLO incorporates this idea as well.  DSSD took the idea a step further, using another kind of neural network called a "deconvolution".  A deconvolution layer is like a convolution layer, but it produces a larger image than the one it started wtih.  It's not actually possible to "add" information as the reverse of throwing away irrelevant information, so I think of a deconvolution as rearranging information.  A quick note: a pedant would insist on pointing out that this is [not a "deconvolution"](https://www.quora.com/What-is-the-difference-between-Deconvolution-Upsampling-Unpooling-and-Convolutional-Sparse-Coding) in mathematical terminology, but a "convolution with fractional strides" or a "transpose convolution".

[DSSD](https://arxiv.org/abs/1701.06659) starts with [SSD](https://arxiv.org/abs/1512.02325) (a well-regarded CNN for object detection) and then adds a few deconvolution layers to improve performance with small and tightly packed objects.  YOLOv2 and SSD have similar performance, but I have more experience with the YOLO models, so I decided to apply the same principle to YOLOv2 and apply it to satellite images.  The deconvolution layers allow us to use a finer grid when placing our bounding box proposals, while still seeing the broader context.

### YOLOv2
![YOLOv2 Model Structure]({{ site.baseurl }}/assets/images/yolod/yolo.png){: width="54%"}

### YOLOD
![YOLOD Model Structure]({{ site.baseurl }}/assets/images/yolod/yolod.png){: width="54%"}

The dotted line represents passthrough, where some information skips ahead in the neural network.  You be wondering how these networks passthrough from a larger layer to a smaller layer.  I'm just going to say "space to depth" and leave it at that.  There's a lot of complexity lurking behind everything in this blog post, and I'm not going to go into it.

I'm skipping over a lot of detail.  Practically speaking, there was a lot of tweaking and tuning involved in getting the structure of the network right.  Details like the depth of the deconvolution layers, the quantity and shapes of anchor boxes, and the non-maximum suppression parameters.  These are called "hyperparameters" and there's really no way to know ahead of time what's going to work best so I had to run experiments to optimize them.  On top of that, detection neural networks have more hyperparameters than most other types, so that means there are more possible combinations to test out.  Fortunately, we have four NVIDIA GP100 GPUs for training neural networks at Radiant Solutions.  These are absolute top-of-the-line graphics cars, and they let me test out stacks of hypotheses quickly.  I used all four GP-100s for about 3 weeks in order to find a good combination of hypterparameters.

The end result of adding the deconvolution layers is that we do detection on a finer grid with more grid cells.  You can see how that affects things if you look at these examples of an image with a 13x13 grid (YOLOv2) and a 26x26 grid (YOLOD).

| Image with 13x13 Grid   | Image with 26x26 Grid |
| :-------: |:--------------:|
| ![Image with 13x13 Grid]({{ site.baseurl }}/assets/images/yolod/grid13.png){: width="66%"}  | ![Image with 26x26 Grid]({{ site.baseurl }}/assets/images/yolod/grid26.png){: width="66%"} |

In the 13x13 grid, we'll sometimes get multiple cars in the same grid cell, but this happens much less frequently in the 26x26 grid.


# Results

Now I'll show some results so you can see how much this improved the performance with cars.  I picked a location in our validation areas that has a bunch of cars packed together.  I'll show you the ground truth markings, car detections with YOLOv2, and car detections with YOLOD.

### Ground Truth  
![Ground Truth]({{ site.baseurl }}/assets/images/yolod/targets.png){: width="45%"}

### YOLOv2  
![YOLOv2]({{ site.baseurl }}/assets/images/yolod/yolo_detects.png){: width="45%"}

### YOLOD  
![YOLOD]({{ site.baseurl }}/assets/images/yolod/yolod_detects.png){: width="45%"}

You can see that both models caught most of the cars, even really dark ones in the shadows, but that YOLOD performs much better in the area on the left where cars are packed together.  By the way, you might notice a white vehicle that's not marked in the ground truth.  This vehicle is actually marked in our ground truth, but it is marked as a bus so it wasn't included in this visualization

Now I'll quantify the performance improvement.  There are 11,083 targets in the validation areas, so a perfect model would have 11,083 true positives, 0 false positives, and 0 false negatives.  Even YOLOD isn't perfect though.

| Model   | True Positive Count | False Positive Count | False Negative Count | Precision | Recall | F1   |
| ------- |--------------:| --------------:|---------------:|--:|--:|--:|
| YOLOv2  |         6,328 |          4,187 |          4,755 |     0.60 |   0.57 | 0.59 |
| YOLOD   |         7,270 |          3,151 |          3,813 |     0.70 |   0.66 | 0.68 |



You can see an improvement in performance with the YOLOD model.  You can't see them all, but this YOLOD and the YOLOv2 model actually have 54 classes.  The "cars" that the model is actually a composite class consisting of cars, pickup trucks, cargo trucks, and truck tractors.  I'm not going to show validation results for all 54 categories, but it's interesting to note that YOLOD did not outperform YOLOv2 on all the classes.  YOLOD was better for classes that come tightly packed and that have many training examples, But YOLOv2 outperformed YOLOD in other cases, such as boats.


ACTUALLY I SHOULD REPORT ALL 54



